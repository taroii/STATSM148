{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data New Embeddings and Time Features\n",
    "### Team Name : Data Crew\n",
    "\n",
    "In this notebook we are getting more features 4 of time and we are implementing another way of getting the embeddings for the two columns of sequences first_20_events and time_since_last_event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Librarie for Preprocessing (By Us)\n",
    "from utils import *\n",
    "\n",
    "# Import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "#random.seed(2024)\n",
    "np.random.seed(2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Retrieval\n",
    "\n",
    "We retrieve the data from the original source, i.e. the one that is uncleaned and unprocessed. Also we retrive the event definition dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('../../1. Data/export.csv')   # This is the original data\n",
    "# data = pd.read_csv('../../1. Data/smaller_sample.csv')  # This is a smaller sample\n",
    "# event_defs = pd.read_csv('../../1. Data/Event+Definitions.csv')  # This is the event dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **-> Execute only if working with the original dataset**\n",
    "\n",
    "As the original dataset does not have some already-merged variables then we have to manually do it. This takes approximately 31 secs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# event_defs.drop(columns=['event_name'], inplace=True)\n",
    "# event_defs.rename(columns={'event_definition_id':'ed_id'}, inplace=True)\n",
    "# data = pd.merge(data, event_defs, on='ed_id', how='left')\n",
    "# event_defs = pd.read_csv('../../1. Data/Event+Definitions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **-> Execute only if working with the sample dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we call the **get_classification_dataset** function with *n_events = $5$* (this parameter could be changed everytime and depends only in the number of sequential events we would like to consider in the last **first_n_events** column). This takes approx **2:07** mins to run (using the smaller_sample data set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number_events_fixed = 20\n",
    "# col_name = 'first_' + str(number_events_fixed) +'_events'\n",
    "\n",
    "# df = get_classification_dataset(data, event_defs, n_events=number_events_fixed)\n",
    "# df.reindex(sorted(df.columns), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore we reset the index and assigned the current index (which are the cust_ids as a new column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cust_ids = df.index\n",
    "# cust_ids = [x[0] for x in cust_ids]\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "# df['customer_id'] = cust_ids\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **-> Execute only if working with the original already-preprocessed dataset (or already preprocessed the first section)**\n",
    "\n",
    "This option was not considered in the beginning as the data set that we are reading is the dataset that it was supposed to be returned by the code in the section **-> Execute only if working with the original dataset**, but due to a time consuming issue, we ran that section once and saved the dataset in a csv file, which is the file we are reading in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean added\n",
      "std added\n",
      "max added\n",
      "Predicting embeddings for time...\n",
      "52031/52031 [==============================] - 134s 3ms/step\n",
      "Predicting embeddings for events...\n",
      "52031/52031 [==============================] - 131s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "def embeddings(df, col_name = 'first_20_events'):\n",
    "    sequences_evs = df[col_name].apply(lambda x: np.array(x)).to_numpy()\n",
    "    sequences_times = df['time_since_last_event'].apply(lambda x: np.array(x)).to_numpy()\n",
    "\n",
    "    padded_time_events = np.vstack(sequences_evs)\n",
    "    padded_time_waits = np.vstack(sequences_times)\n",
    "    \n",
    "    max_seq_length = 20\n",
    "    embedding_dim = 5\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=128, input_shape=(max_seq_length, 1), return_sequences=False))  \n",
    "    model.add(Dense(units=64, activation='relu'))  \n",
    "    model.add(Dense(units=32, activation='relu'))  \n",
    "    model.add(Dense(units=embedding_dim, activation='relu'))  \n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "    print('Predicting embeddings for time...')\n",
    "    time_embeddings = model.predict(padded_time_waits)\n",
    "\n",
    "    print('Predicting embeddings for events...')\n",
    "    event_embeddings = model.predict(padded_time_events)\n",
    "\n",
    "    event_embd = pd.DataFrame(event_embeddings, columns=[f'event_embd_{i}' for i in range(5)])\n",
    "    time_embd = pd.DataFrame(time_embeddings, columns=[f'time_embd_{i}' for i in range(5)])\n",
    "\n",
    "    df.drop(columns=[col_name, 'time_since_last_event'], inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    new_dfx = pd.concat([df, time_embd, event_embd], axis=1)\n",
    "    return new_dfx\n",
    "\n",
    "def vec_to_list(event_list):\n",
    "    event_list = event_list.replace('[', '').replace(']', '').split()\n",
    "    event_list = [int(float(x)) for x in event_list]\n",
    "    return event_list\n",
    "\n",
    "def preprocessing_steps_embedding(data):\n",
    "    df = data.copy()\n",
    "    \n",
    "    # Dropping columns that introduce bias to the model\n",
    "    df = df.drop(columns=['Unnamed: 1', 'downpayment_cleared', 'first_purchase',\n",
    "                          'max_milestone', 'downpayment_received', 'account_activitation', 'customer_id'])\n",
    "    \n",
    "    # We set this parameters for future interactions with these features\n",
    "    number_events_fixed = 20\n",
    "    col_name = 'first_' + str(number_events_fixed) +'_events'\n",
    "    \n",
    "    # As we are reading the data from a csv, the list of events is read as a string\n",
    "    # and therefore we need to transform this type of data\n",
    "    result = []\n",
    "    for item in list(df[col_name]):\n",
    "        numbers = [int(num) for num in item.replace('[', '').replace(']', '').split()]\n",
    "        #numbers += [0] * (number_events_fixed - len(numbers))    \n",
    "        result.append(numbers)\n",
    "    result2 = []\n",
    "    for item in list(df['time_since_last_event']):\n",
    "        numbers = [float(num) for num in item.replace('[', '').replace(']', '').split()]\n",
    "        #numbers += [0] * (number_events_fixed - len(numbers))    \n",
    "        result2.append(numbers)\n",
    "    \n",
    "    # We have the columns again in a list type\n",
    "    df[col_name] = result\n",
    "    df['time_since_last_event'] = result2\n",
    "    \n",
    "    # Here we set all the float columns to numbers 0 or 1\n",
    "    df = df.astype({col: 'float' for col in df.columns[:-2]})\n",
    "    \n",
    "    # We realized the dataset in the initial_devices had nan values\n",
    "    df = df.dropna(axis=0)\n",
    "    \n",
    "    # Adding more features\n",
    "    df['total_time_spent'] = df['time_since_last_event'].apply(lambda x: np.sum(x))\n",
    "    df['time_mean'] = df['time_since_last_event'].apply(lambda x: np.mean(x))\n",
    "    print('mean added')\n",
    "    df['time_std'] = df['time_since_last_event'].apply(lambda x: np.std(x))\n",
    "    print('std added')\n",
    "    df['time_max'] = df['time_since_last_event'].apply(lambda x: np.max(x))\n",
    "    print('max added')\n",
    "    \n",
    "    # We create and generate the embeddings\n",
    "    # we drop the first_20_events and the time_since_last_event column\n",
    "    # but we kept the embeddings\n",
    "    df = embeddings(df)\n",
    "\n",
    "    # Getting the dataset balanced\n",
    "    df_0, df_1 = df[df.order_ships == 0], df[df.order_ships == 1]\n",
    "    df_0 = df_0.sample(n=len(df_1), random_state=2024)\n",
    "    # df_1 = df_1.sample(n=(len(df_0)), replace=True)\n",
    "    df_balanced = pd.concat([df_0, df_1], axis=0).reset_index(drop=True)\n",
    "\n",
    "    # shuffle\n",
    "    df_balanced = df_balanced.sample(frac=1)\n",
    "\n",
    "    df_X = df_balanced.drop(columns='order_ships')\n",
    "    target = df_balanced.order_ships\n",
    "    ori_df = df.drop(columns='order_ships')\n",
    "    ori_target = df.order_ships\n",
    "\n",
    "    boolean_col = ['discover', 'one_more_journey', 'approved_credit', 'has_prospecting', 'has_pre_application']\n",
    "\n",
    "    for col in boolean_col:\n",
    "        df_X[col] = [1 if val == True else 0 for val in df_X[col]]\n",
    "        ori_df[col] = [1 if val == True else 0 for val in ori_df[col]]\n",
    "\n",
    "    return ori_df, ori_target, df_X, target\n",
    "\n",
    "# Read in preprocessed original dataset\n",
    "df = pd.read_csv('../../1. Data/export_n_20.csv')\n",
    "df.reindex(sorted(df.columns), axis=1)\n",
    "df.head()\n",
    "\n",
    "# Preprocess\n",
    "ori_data, ori_target, df, target = preprocessing_steps_embedding(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the dataset and reading again (memory and time issues)\n",
    "\n",
    "Due to the limited computational resources and the time consuming issue, we decided to save the dataset in a **.csv** file and then read it again in order to avoid this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.concat([ori_data, ori_target], axis=1).to_csv('../../1. Data/data_with_embeddings.csv', index=False)\n",
    "#pd.concat([df, target], axis=1).to_csv('../../1. Data/data_with_embeddings_balanced.csv', index=False) # This data was made in order to train the models and test with the whole dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kernelsote",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preprocessing\n",
    "### Team : Data Crew\n",
    "\n",
    "This notebook was created in order to cleand a make a preprocessing of the data in order to make it ready for the model development. The data that we use in this notebook has the original structure as provided by Fingerhut. Additionally, we are creating another **.py** that will store all the functions that we are using here in order to make the code more readable and easy to understand and make the cleaning and preprocessing in one single local package.\n",
    "\n",
    "Once the cleaning and the preprocessing in this notebook is tested and preprocessed we are going to update the **utils.py**. This file can be imported in the current session in other notebooks by just using the following command:\n",
    "\n",
    "```python\n",
    "from utils import *\n",
    "```\n",
    "\n",
    "Update: We are also performing some feature engineering that is tested and then added to the **utils.py** file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Own Libraries\n",
    "from utils import *\n",
    "\n",
    "# Dictionary\n",
    "import dask.dataframe as dd\n",
    "\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "This section contains every concerc about the data retrieval and reading. This because initially we were having some issues with the time that the algorithms, cleaning and feature engineering sections were taking to run. So we decided to try a different approach by parallelizing some tasks but we didn't suceed and we ended up doing it the traditional way.\n",
    "\n",
    "Results : The final expected data set was returned by the following functions in 2:30 hours. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data\n",
    "\n",
    "We can work with either the smaller sample or the original dataset but in this case we decided to work with the smaller sample to run the tests and process faster and make the debugging easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DASK APRROACH (PARALLEL)\n",
    "\n",
    "# #data = dd.read_csv('../../1. Data/smaller_sample.csv', assume_missing=True)\n",
    "# data = dd.read_csv('../../1. Data/export.csv', assume_missing=True)\n",
    "# event_defs = dd.read_csv('../../1. Data/Event+Definitions.csv')\n",
    "# #data = data.compute()\n",
    "# #event_defs = event_defs.compute()\n",
    "# #data.head()\n",
    "\n",
    "# PANDAS\n",
    "data = pd.read_csv('../../1. Data/smaller_sample.csv')   # this is the smaller sample data set\n",
    "#data = pd.read_csv('../../1. Data/export.csv')   # this is the original data set\n",
    "event_defs = pd.read_csv('../../1. Data/Event+Definitions.csv')   # dictionary for the events\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The following code needs to be run in case we are working with the original data set either with dask or with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DASK\n",
    "# event_defs = event_defs.rename(columns={'event_definition_id': 'ed_id'})\n",
    "# data = dd.merge(data, event_defs.drop(columns=['event_name']), on='ed_id', how='left')\n",
    "#data.compute()\n",
    "\n",
    "# PANDAS\n",
    "# event_defs = event_defs.rename(columns={'event_definition_id': 'ed_id'})\n",
    "# data = pd.merge(data, event_defs.drop(columns=['event_name']), on='ed_id', how='left')\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results between using DASK and PANDAS\n",
    "#### Benchmark\n",
    "\n",
    "PANDAS:\n",
    "* Reading and showing the head of the data takes : 38.6s\n",
    "* Events definition dataset renaming columns and merging data takes : 19.8s\n",
    "* TOTAL : 58.4s\n",
    "\n",
    "* ALL the cleaning function : 13mins.\n",
    "\n",
    "DASK:\n",
    "* Reading and showing the head of the data takes and Events definition dataset renaming columns and merging data takes : 53.4s\n",
    "* TOTAL : 53.4s\n",
    "\n",
    "* ALL the cleaning function : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Applying the cleaning functions that we have in the utils package, this cleaning function is the **fingerhut_data_cleaner**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fingerhut_data_cleaner_test(og_df, defs):\n",
    "    # Dropping duplicate (ignoring journey steps variable)\n",
    "    df = og_df[['customer_id',\n",
    "             'account_id',\n",
    "             'ed_id',\n",
    "             'event_name',\n",
    "             'event_timestamp',\n",
    "             'journey_steps_until_end',\n",
    "             'milestone_number',\n",
    "             'journey_id',]]\n",
    "    \n",
    "    # Assuming df is your Dask DataFrame\n",
    "    df = df.assign(milestone_number=df['milestone_number'].copy().fillna(0))\n",
    "\n",
    "    df = df.drop_duplicates(subset=['customer_id', 'account_id', 'ed_id', 'event_name', 'event_timestamp'])\n",
    "    df = df.reset_index(drop=True) # re-indexing    \n",
    "    df = df.compute()\n",
    "    \n",
    "    # Re-adding journey_steps_until_end (Axel's way)\n",
    "    j_steps = df['journey_steps_until_end']\n",
    "    s_corrected = correct_sequences(j_steps)\n",
    "    df = df.assign(journey_steps_until_end=s_corrected)\n",
    "    \n",
    "    # Convert event_timestamps to datetime objects\n",
    "    #df['event_timestamp'] = dd.to_datetime(df['event_timestamp'], format='mixed')\n",
    "    df = df.assign(event_timestamp=dd.to_datetime(df['event_timestamp'], format='mixed'))\n",
    "        \n",
    "    # Adding a `stage` variable based on the event definitions\n",
    "    df_stages = defs[['event_name', 'stage']]\n",
    "    \n",
    "    df = dd.merge(df, df_stages, on ='event_name', how = 'left')\n",
    "    \n",
    "    # Setting positive values for account_ids\n",
    "    #df['account_id'] = remove_if(df, 'account_id')\n",
    "    df = df.assign(account_id=remove_if(df, 'account_id'))\n",
    "    \n",
    "    # Setting positive values for customer_ids\n",
    "    #df['customer_id'] = remove_if(df, 'customer_id')\n",
    "    df = df.assign(customer_id=remove_if(df, 'customer_id'))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fingerhut_data_cleaner(data, event_defs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Verifying if the data is clean and the sequences are corrected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_correct_sequence(df['journey_steps_until_end'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Deleting promotion created event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = list(df[df['event_name'] == 'promotion_created'].index)\n",
    "\n",
    "# DROP THE INDEX OF THIS QUERY IN THE DATASET\n",
    "df.drop(idxs, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequences (Obtaining sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining the sequences with the correct states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Function for the sequences and the states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_sequences = split_sequences(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Assigning the probabilities for each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_sequences[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of the length of the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lengths = []\n",
    "# for seq in result_sequences:\n",
    "#     lengths.append(len(seq))\n",
    "    \n",
    "# plt.figure(figsize=(12, 6))\n",
    "# sns.histplot(lengths, kde=True)\n",
    "# plt.title('Length of Sequences')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating New Data Set for Binary Classification\n",
    "\n",
    "* By making it suitable for clustering/analysis/classification of the customers without the explicit sequences\n",
    "\n",
    "My proposal is to create a new dataset with the customer_ids where each instance corresponds to an only customer where the features are descriptive features about the sequences and the stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unique_cust = len(df['customer_id'].unique())\n",
    "print(f'Number of unique customers: {num_unique_cust}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas of more features to be added:   \n",
    "* Avg length of the sequence (avg in case has more than one, otherwise the length of the unique sequence) ✅\n",
    "* Has more than one journey ✅\n",
    "* Number of journeys ✅\n",
    "* Has order shipped ✅\n",
    "* Highest milestone reached ✅\n",
    "* Has credit approved ✅\n",
    "* Has first purchase ✅\n",
    "* Has down payment ✅\n",
    "* Has order shipped ✅\n",
    "* max number of journey_steps_until_end ✅\n",
    "* Most repeated event in the journey ✅\n",
    "* Time between the application and first purchase, then time from the first purchase to down payment and then down payment to order shipped\n",
    "\n",
    "* number of attempts (applying for a credit)\n",
    "* Returning customer\n",
    "* Has an ideal journey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to use Dask for improving the vectorized operations and the parallelism\n",
    "\n",
    "This particular library will be very useful in case we handle operations with the original data.\n",
    "\n",
    "Ofc the following **benchmark** is designed thinking on the original data and not the sample, because as we may not parallelize the operations with very few data could be even slower or not worth it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import time\n",
    "# import dask.array as da\n",
    "\n",
    "df_dask = dd.from_pandas(df, npartitions=4)\n",
    "\n",
    "# Testing the parallel computation\n",
    "time_init = time.time()\n",
    "result = df_dask['journey_steps_until_end'] * df_dask['ed_id'].sum()  # Compute sum(x * y) in parallel\n",
    "time_end = time.time()\n",
    "\n",
    "print(result.compute())\n",
    "print(f'Time: {time_end - time_init}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_init = time.time()\n",
    "result = df['journey_steps_until_end'] * df['ed_id'].sum()  # Compute sum(x * y) in parallel\n",
    "time_end = time.time()\n",
    "\n",
    "print(result)\n",
    "print(f'Time: {time_end - time_init}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def number_journeys_and_max(cus_df):\n",
    "#     \"\"\"Function to check the number of journeys in a sequence\n",
    "\n",
    "#     Args:\n",
    "#         seq (list): List of values\n",
    "\n",
    "#     Returns:\n",
    "#         int: Number of journeys in the sequence\n",
    "#     \"\"\"\n",
    "#     j_steps = cus_df['journey_steps_until_end']\n",
    "#     ones = [i for i, x in enumerate(j_steps) if x == 1]\n",
    "#     return len(ones), max(j_steps)\n",
    "\n",
    "# def has_discover(cust_df):\n",
    "#     \"\"\"Function to check if a sequence has the discovery event\n",
    "\n",
    "#     Args:\n",
    "#         cust_df (pd.DataFrame): Dataset of a certain customer (not all the dataset, just one customer)\n",
    "\n",
    "#     Returns:\n",
    "#         bool: True if the sequence has the discovery event, False otherwise\n",
    "#     \"\"\"\n",
    "#     return 'Discover' in list(cust_df['stage'])\n",
    "\n",
    "# def number_accounts(cust_df):\n",
    "#     \"\"\"Function to add the number of accounts to the dataset\n",
    "\n",
    "#     Args:\n",
    "#         cust_df (pd.DataFrame): Dataset of a certain customer (not all the dataset, just one customer)\n",
    "\n",
    "#     Returns:\n",
    "#         pd.DataFrame: Dataset with the number of accounts in a new column\n",
    "#     \"\"\"\n",
    "#     return cust_df['account_id'].nunique()\n",
    "\n",
    "# def has_more_one_journey(j_steps):\n",
    "#     \"\"\"Function to check if a sequence has repeated values\n",
    "\n",
    "#     Args:\n",
    "#         seq (list): List of values\n",
    "\n",
    "#     Returns:\n",
    "#         bool: True if there are repeated values, False otherwise\n",
    "#     \"\"\"\n",
    "#     return len(j_steps) != len(set(j_steps))\n",
    "\n",
    "# def most_repeated_event(cust_df):\n",
    "#     \"\"\"Function that returns the most repeated event in a sequence\n",
    "\n",
    "#     Args:\n",
    "#         cust_df (pd.DataFrame): Dataset of a certain customer (not all the dataset, just one customer)\n",
    "\n",
    "#     Returns:\n",
    "#         str: The most repeated event in the sequence\n",
    "#     \"\"\"\n",
    "#     return cust_df['ed_id'].mode()[0]\n",
    "\n",
    "# def average_length_seq(cust_df):\n",
    "#     \"\"\"Function to add the average length of the sequences to the dataset\n",
    "\n",
    "#     Args:\n",
    "#         cust_df (pd.DataFrame): Dataset of a certain customer (not all the dataset, just one customer)\n",
    "\n",
    "#     Returns:\n",
    "#         pd.DataFrame: Dataset with the average length of the sequences in a new column\n",
    "#     \"\"\"\n",
    "#     new_df = cust_df.copy()\n",
    "#     # Split the sequences\n",
    "#     sequences = split_sequences(new_df)\n",
    "#     return np.mean([len(seq) for seq in sequences])\n",
    "\n",
    "# def has_prospecting(cust_df):\n",
    "#     \"\"\"Function to check if a sequence has the prospecting event\n",
    "\n",
    "#     Args:\n",
    "#         cust_df (pd.DataFrame): Dataset of a certain customer (not all the dataset, just one customer)\n",
    "\n",
    "#     Returns:\n",
    "#         bool: True if the sequence has the prospecting event, False otherwise\n",
    "#     \"\"\"\n",
    "#     evnts = list(cust_df['ed_id'])\n",
    "#     return 20 in evnts or 21 in evnts or 24 in evnts\n",
    "\n",
    "# def has_pre_application(cust_df):\n",
    "#     \"\"\"Function to check if a sequence has the pre-application event\n",
    "\n",
    "#     Args:\n",
    "#         cust_df (pd.DataFrame): Dataset of a certain customer (not all the dataset, just one customer)\n",
    "\n",
    "#     Returns:\n",
    "#         bool: True if the sequence has the pre-application event, False otherwise\n",
    "#     \"\"\"\n",
    "#     return 22 in list(cust_df['ed_id'])\n",
    "\n",
    "# def initial_device(cust_df):\n",
    "#     \"\"\"Function to get the initial device of a customer\n",
    "#     \"\"\"\n",
    "#     events = set(cust_df['event_name'])\n",
    "#     phone = ['phone' in event for event in events]\n",
    "#     web = ['web' in event for event in events]\n",
    "    \n",
    "#     if np.array(phone).any() and np.array(web).any():\n",
    "#         return 3\n",
    "#     elif np.array(phone).any():\n",
    "#         return 1\n",
    "#     elif np.array(web).any():\n",
    "#         return 2\n",
    "    \n",
    "# def has_approved(cust_df):\n",
    "#     \"\"\"Function to check if a sequence has the approved event\n",
    "\n",
    "#     Args:\n",
    "#         cust_df (pd.DataFrame): Dataset of a certain customer (not all the dataset, just one customer)\n",
    "\n",
    "#     Returns:\n",
    "#         bool: True if the sequence has the approved event, False otherwise\n",
    "#     \"\"\"\n",
    "#     x = set(cust_df['ed_id'])\n",
    "#     return 15 in x or 12 in x\n",
    "\n",
    "# def get_first_n_events(cust_df, n = 10):\n",
    "#     \"\"\"Function that returns the first 10 events of a sequence\n",
    "\n",
    "#     Args:\n",
    "#         cust_df (pd.DataFrame): Dataset of a certain customer (not all the dataset, just one customer)\n",
    "\n",
    "#     Returns:\n",
    "#         list: The first 10 events of the sequence, padded with np.nan if necessary\n",
    "#     \"\"\"\n",
    "#     events = cust_df['ed_id'].head(n).tolist()\n",
    "#     # Pad with np.nan if the sequence has fewer than 10 events\n",
    "#     #events += [np.nan] * (10 - len(events))\n",
    "#     return np.array(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def which_milestones(cust_df):\n",
    "#     \"\"\"Function that returns in a tuple in the following sequence the next statemens:\n",
    "#     - If the customer has applied for credit and it has been approved (milestone 1)\n",
    "#     - If the customer has first purchase (milestone 2)\n",
    "#     - If the customer has account activitation (milestone 3)\n",
    "#     - If the customer has downpayment received (milestone 4)\n",
    "#     - If the customer has downpayment cleared (milestone 5)\n",
    "#     - If the customer has order shipped (milestone 6)\n",
    "\n",
    "#     Args:\n",
    "#         cust_df (_type_): _description_\n",
    "#     \"\"\"\n",
    "#     milestones = set(cust_df['milestone_number'].unique())\n",
    "#     max_milestone = max(milestones)\n",
    "#     return (1 in milestones, 2 in milestones, 3 in milestones, 4 in milestones, 5 in milestones, 6 in milestones), max_milestone\n",
    "\n",
    "# # Functions for time\n",
    "# def get_idxs(cust_df, stage, milestone = -1):\n",
    "#     \"\"\"Function to get the indexes of a certain stage\n",
    "\n",
    "#     Args:\n",
    "#         cust_df (pd.DataFrame): Dataset of a certain customer (not all the dataset, just one customer)\n",
    "\n",
    "#     Returns:\n",
    "#         list: List with the indexes of a certain stage\n",
    "#     \"\"\"\n",
    "#     if milestone != -1:\n",
    "#         return list(cust_df[cust_df['milestone_number'] == milestone].index)\n",
    "    \n",
    "#     return list(cust_df[cust_df['stage'] == stage].index)\n",
    "\n",
    "# def time_in_discover(cust_df, seconds_differences):\n",
    "#     \"\"\"Function to calculate the time between events\n",
    "\n",
    "#     Args:\n",
    "#         cust_df (pd.DataFrame): Dataset of a certain customer (not all the dataset, just one customer)\n",
    "\n",
    "#     Returns:\n",
    "#         list: List with the time between events\n",
    "#     \"\"\"\n",
    "#     idxs = get_idxs(cust_df, 'Discover')\n",
    "    \n",
    "#     time_in = []\n",
    "#     for idx in idxs:\n",
    "#         if idx + 1 < len(seconds_differences):\n",
    "#             time_in.append(seconds_differences[idx + 1])\n",
    "#         else:\n",
    "#             time_in.append(0)\n",
    "#     return sum(time_in)\n",
    "\n",
    "# def time_in_apply(cust_df, seconds_differences):\n",
    "#     \"\"\"Function to calculate the time between events\n",
    "\n",
    "#     Args:\n",
    "#         cust_df (pd.DataFrame): Dataset of a certain customer (not all the dataset, just one customer)\n",
    "\n",
    "#     Returns:\n",
    "#         list: List with the time between events\n",
    "#     \"\"\"\n",
    "#     idxs = get_idxs(cust_df, 'Apply for Credit')\n",
    "    \n",
    "#     time_in = []\n",
    "#     for idx in idxs:\n",
    "#         if idx + 1 < len(seconds_differences):\n",
    "#             time_in.append(seconds_differences[idx + 1])\n",
    "#         else:\n",
    "#             time_in.append(0)\n",
    "#     return sum(time_in)\n",
    "\n",
    "# def time_reach_milestone1(cust_df, seconds_differences):\n",
    "#     \"\"\"Function to calculate the time between events\n",
    "\n",
    "#     Args:\n",
    "#         cust_df (pd.DataFrame): Dataset of a certain customer (not all the dataset, just one customer)\n",
    "\n",
    "#     Returns:\n",
    "#         list: List with the time between events\n",
    "#     \"\"\"\n",
    "#     idxs = get_idxs(cust_df, 'Apply for Credit', 1)\n",
    "    \n",
    "#     # sum all the times before the milestone\n",
    "#     return sum(seconds_differences[1:idxs[0]+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1. Groupby Approach:     \n",
    "    This avoids repeated filtering and improves efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def group_by_approach(cust_df):\n",
    "#     cust_df = cust_df.reset_index(drop=True)\n",
    "#     # applying all the functions to get the data\n",
    "#     num_journeys, max_journey = number_journeys_and_max(cust_df)\n",
    "#     discover = has_discover(cust_df)\n",
    "#     numb_accs = number_accounts(cust_df)\n",
    "#     more_one_journey = has_more_one_journey(cust_df['journey_steps_until_end'])\n",
    "#     repeated_event = most_repeated_event(cust_df)\n",
    "#     avg_length_journey = average_length_seq(cust_df)\n",
    "#     has_pros = has_prospecting(cust_df)\n",
    "#     pre_applic = has_pre_application(cust_df)\n",
    "#     device = initial_device(cust_df)\n",
    "#     x = cust_df['event_timestamp'].diff().dt.total_seconds().tolist()\n",
    "#     time_disc = time_in_discover(cust_df, x)\n",
    "#     time_apply = time_in_apply(cust_df, x)\n",
    "#     # time_milestone1 = time_reach_milestone1(cust_df, x)\n",
    "    \n",
    "#     milestones, max_milestone = which_milestones(cust_df)\n",
    "    \n",
    "#     # Creating the new data frame\n",
    "#     new_df = pd.DataFrame({'num_journeys': num_journeys,\n",
    "#                            'max_journey': max_journey,\n",
    "#                            'discover': discover, \n",
    "#                            'number_accounts': numb_accs,\n",
    "#                            'one_more_journey': more_one_journey,\n",
    "#                            'most_repeated_event': repeated_event,\n",
    "#                            'average_length_seq': avg_length_journey,\n",
    "#                            'approved_credit': milestones[0],\n",
    "#                            'first_purchase': milestones[1],\n",
    "#                            'account_activitation': milestones[2],\n",
    "#                            'downpayment_received': milestones[3],\n",
    "#                            'downpayment_cleared': milestones[4],\n",
    "#                            'order_ships': milestones[5],\n",
    "#                            'max_milestone': max_milestone,\n",
    "#                             'has_prospecting': has_pros,\n",
    "#                             'has_pre_application': pre_applic,\n",
    "#                             'initial_device': device,\n",
    "#                             'time_in_discover': time_disc,\n",
    "#                             'time_in_apply': time_apply,\n",
    "#                             #'time_reach_milestone_1': time_milestone1,\n",
    "#                            'index':[0]})\n",
    "#     return new_df    \n",
    "\n",
    "# def get_classification_dataset(data, event_defs, n_events = 10):\n",
    "#     df = fingerhut_data_cleaner(data, event_defs)\n",
    "#     # drop the promotion_created event\n",
    "#     idxs = list(df[df['event_name'] == 'promotion_created'].index)\n",
    "#     df.drop(idxs, inplace=True)\n",
    "#     df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "#     # Grouping by the customer id and gathering the data\n",
    "#     new_df = df.groupby('customer_id').apply(group_by_approach)\n",
    "#     new_df.drop(columns=['index'], inplace=True)\n",
    "    \n",
    "#     # Adding the first n events\n",
    "#     x = list(df.groupby('customer_id').apply(get_first_n_events, n = n_events))\n",
    "#     new_df['first_' + str(n_events) + '_events'] = x\n",
    "    \n",
    "#     return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Approx Time : 1 minute or less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New dataset\n",
    "new_df = get_classification_dataset(data, event_defs, n_events = 3)\n",
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This next command keeps the dataset in your local working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('new_dataset_original_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "The following analysis is pending to be done.\n",
    "\n",
    "Good ideas for further analysis:\n",
    "\n",
    "* Go further in the analysis in the people that had applied for a credit and did not get it\n",
    "\n",
    "* Figure out if we add rows with the same event name in a very small (but different) time difference (e.g. 1 second) to the dropping criteria. Example:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kernelsote",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

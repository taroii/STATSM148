{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('smaller_sample.csv')\n",
    "events = pd.read_csv('Event Definitions.csv')\n",
    "# data.customer_id = list(range(0, len(data)))\n",
    "df = fingerhut_data_cleaner(data, events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = list(df[df['event_name'] == 'promotion_created'].index)\n",
    "df.drop(idxs, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_downpayment_cleared(df):\n",
    "    downpaymen_cleared = df.groupby('customer_id')['ed_id'].apply(lambda x: 27 in x.values).reset_index(name='downpayment_cleared')\n",
    "    return pd.merge(df, downpaymen_cleared, on='customer_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add some other features. Will need more in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_n_accounts(df)\n",
    "df = add_has_discover(df)\n",
    "df = add_downpayment_cleared(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter customers with at least 10 events and downpayment not cleared in the first 10 events. Want to see if models can predict whether or not a customer clears downpayment by only looking at first 10 actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(\n",
    "        ['customer_id', 'journey_steps_until_end']\n",
    "    ).groupby(['customer_id'], sort=False).agg(\n",
    "        {\n",
    "            'ed_id': lambda x: list(x)[:10],\n",
    "            'journey_steps_until_end': lambda x: list(x)[-1],\n",
    "            'has_discover': 'first',\n",
    "            'downpayment_cleared': 'first',\n",
    "            'n_accounts': 'first',\n",
    "        }\n",
    ")\n",
    "\n",
    "df = df[df.ed_id.apply(lambda x: len(x) == 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = df[df.ed_id.apply(lambda x: 27 in x)].index\n",
    "df.drop(idx, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = df.drop(columns=['downpayment_cleared', 'customer_id'])\n",
    "target = df.downpayment_cleared.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in available data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_with_embeddings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['num_journeys', 'max_journey', 'discover', 'number_accounts',\n",
       "       'one_more_journey', 'most_repeated_event', 'average_length_seq',\n",
       "       'approved_credit', 'order_ships', 'has_prospecting',\n",
       "       'has_pre_application', 'initial_device', 'time_in_discover',\n",
       "       'time_in_apply', 'event_id_0', 'event_id_1', 'event_id_2', 'event_id_3',\n",
       "       'event_id_4', 'time_0', 'time_1', 'time_2', 'time_3', 'time_4'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop(columns=['max_milestone', 'downpayment_cleared', 'first_purchase', \n",
    "# 'downpayment_received', 'account_activitation', 'customer_id'], inplace=True)\n",
    "df = df.dropna(axis=0)\n",
    "\n",
    "df_0, df_1 = df[df.order_ships == 0], df[df.order_ships == 1]\n",
    "df_0 = df_0.sample(n=len(df_1))\n",
    "# df_1 = df_1.sample(n=(len(df_0)), replace=True)\n",
    "df_balanced = pd.concat([df_0, df_1], axis=0).reset_index(drop=True)\n",
    "\n",
    "# shuffle\n",
    "df_balanced = df_balanced.sample(frac=1)\n",
    "\n",
    "df_X = df_balanced.drop(columns='order_ships')\n",
    "target = df_balanced.order_ships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(10, embedding_dim, bias=False)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed event id sequence into smaller dimension\n",
    "event_id = df_X.ed_id\n",
    "event_id = torch.tensor(event_id.to_list()).float()\n",
    "emb = Embedding(5)\n",
    "event_id = emb(event_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression / Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_id_df = pd.DataFrame(event_id.detach().numpy())\n",
    "ori_dfx = df_X.drop(columns='ed_id').reset_index(drop=True)\n",
    "new_dfx = pd.concat([ori_dfx, event_id_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(new_dfx, target, train_size=.8, random_state=123)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X, target, train_size=.8, random_state=2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5602726991013325"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_clf = LogisticRegression(random_state=2024)\n",
    "log_clf.fit(X_train, y_train)\n",
    "prediction = log_clf.predict(X_test)\n",
    "accuracy_score(y_test, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8379299659126124"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = XGBClassifier(n_estimators=200, \n",
    "                    max_depth=10, \n",
    "                    learning_rate=0.1, \n",
    "                    tree_method='approx',\n",
    "                    objective='binary:logistic',)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "preds = clf.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8021382088627208"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_clf = AdaBoostClassifier(n_estimators=200,\n",
    "                             learning_rate=1.0,\n",
    "                             random_state=2024)\n",
    "\n",
    "ada_clf.fit(X_train, y_train)\n",
    "ada_prediction = ada_clf.predict(X_test)\n",
    "accuracy_score(y_test, ada_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8359157111868608"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_clf = GradientBoostingClassifier(n_estimators=200,\n",
    "                                    learning_rate=0.1,\n",
    "                                    loss='log_loss',\n",
    "                                    max_depth=10,\n",
    "                                    tol=1e-5,\n",
    "                                    random_state=2024)\n",
    "\n",
    "gb_clf.fit(X_train, y_train)\n",
    "gb_prediction = gb_clf.predict(X_test)\n",
    "accuracy_score(y_test, gb_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like boosting algorithms (XGBoost, AdaBoost, Gradient Boosting) are achieving better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = df_X.columns.to_list()\n",
    "feature_tensors = [torch.tensor(df_balanced[feature].values).float().unsqueeze(1) for feature in feature_columns]\n",
    "\n",
    "# res = torch.cat([event_id] + feature_tensors, dim=1).to(device)\n",
    "res = torch.cat(feature_tensors, dim=1).to(device)\n",
    "target = torch.tensor(target, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(res, target, train_size=.7, random_state=42)\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "        self.fc2 = nn.Linear(512, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 50, Loss: 31.5605, Validation Loss: 36.6210, Accuracy: 62.1423\n",
      "Epoch 2 / 50, Loss: 58.6207, Validation Loss: 45.9562, Accuracy: 53.4965\n",
      "Epoch 3 / 50, Loss: 55.1724, Validation Loss: 47.4891, Accuracy: 52.3293\n",
      "Epoch 4 / 50, Loss: 48.2759, Validation Loss: 46.3556, Accuracy: 53.2280\n",
      "Epoch 5 / 50, Loss: 51.9932, Validation Loss: 45.7640, Accuracy: 53.6618\n",
      "Epoch 6 / 50, Loss: 44.8276, Validation Loss: 46.0418, Accuracy: 53.6515\n",
      "Epoch 7 / 50, Loss: 44.8276, Validation Loss: 45.0252, Accuracy: 54.4159\n",
      "Epoch 8 / 50, Loss: 51.7241, Validation Loss: 46.2333, Accuracy: 53.5172\n",
      "Epoch 9 / 50, Loss: 44.8276, Validation Loss: 45.3558, Accuracy: 54.2609\n",
      "Epoch 10 / 50, Loss: 37.9310, Validation Loss: 45.5692, Accuracy: 54.0647\n",
      "Epoch 11 / 50, Loss: 34.4828, Validation Loss: 45.1963, Accuracy: 54.2816\n",
      "Epoch 12 / 50, Loss: 41.3793, Validation Loss: 39.2092, Accuracy: 60.3760\n",
      "Epoch 13 / 50, Loss: 48.2759, Validation Loss: 45.2203, Accuracy: 54.4882\n",
      "Epoch 14 / 50, Loss: 31.0345, Validation Loss: 38.2070, Accuracy: 61.4606\n",
      "Epoch 15 / 50, Loss: 44.8276, Validation Loss: 46.6901, Accuracy: 53.1970\n",
      "Epoch 16 / 50, Loss: 44.8276, Validation Loss: 45.4464, Accuracy: 54.3022\n",
      "Epoch 17 / 50, Loss: 31.0345, Validation Loss: 42.5514, Accuracy: 57.0912\n",
      "Epoch 18 / 50, Loss: 37.9310, Validation Loss: 38.3326, Accuracy: 61.4503\n",
      "Epoch 19 / 50, Loss: 41.3793, Validation Loss: 45.2653, Accuracy: 54.5398\n",
      "Epoch 20 / 50, Loss: 41.3793, Validation Loss: 38.4186, Accuracy: 61.2850\n",
      "Epoch 21 / 50, Loss: 41.5846, Validation Loss: 38.1864, Accuracy: 61.5845\n",
      "Epoch 22 / 50, Loss: 41.3793, Validation Loss: 38.1665, Accuracy: 61.5639\n",
      "Epoch 23 / 50, Loss: 34.4828, Validation Loss: 38.0164, Accuracy: 61.7188\n",
      "Epoch 24 / 50, Loss: 24.1379, Validation Loss: 38.7079, Accuracy: 61.0474\n",
      "Epoch 25 / 50, Loss: 31.0345, Validation Loss: 38.6735, Accuracy: 61.1300\n",
      "Epoch 26 / 50, Loss: 41.3793, Validation Loss: 42.9305, Accuracy: 56.9156\n",
      "Epoch 27 / 50, Loss: 31.0345, Validation Loss: 38.1263, Accuracy: 61.7395\n",
      "Epoch 28 / 50, Loss: 37.9310, Validation Loss: 38.1578, Accuracy: 61.6052\n",
      "Epoch 29 / 50, Loss: 31.0345, Validation Loss: 40.1078, Accuracy: 59.6839\n",
      "Epoch 30 / 50, Loss: 34.4828, Validation Loss: 38.8112, Accuracy: 61.0061\n",
      "Epoch 31 / 50, Loss: 31.2830, Validation Loss: 42.9651, Accuracy: 56.7503\n",
      "Epoch 32 / 50, Loss: 58.6207, Validation Loss: 43.1687, Accuracy: 56.7090\n",
      "Epoch 33 / 50, Loss: 48.2759, Validation Loss: 43.2353, Accuracy: 56.6161\n",
      "Epoch 34 / 50, Loss: 44.8276, Validation Loss: 46.7483, Accuracy: 53.1867\n",
      "Epoch 35 / 50, Loss: 51.7241, Validation Loss: 46.0275, Accuracy: 53.7858\n",
      "Epoch 36 / 50, Loss: 37.9310, Validation Loss: 45.8531, Accuracy: 54.0130\n",
      "Epoch 37 / 50, Loss: 44.8276, Validation Loss: 45.5696, Accuracy: 54.2609\n",
      "Epoch 38 / 50, Loss: 51.7241, Validation Loss: 46.1353, Accuracy: 53.7341\n",
      "Epoch 39 / 50, Loss: 46.5233, Validation Loss: 46.0772, Accuracy: 53.7858\n",
      "Epoch 40 / 50, Loss: 51.7241, Validation Loss: 45.8940, Accuracy: 53.9304\n",
      "Epoch 41 / 50, Loss: 62.0690, Validation Loss: 45.4915, Accuracy: 54.3745\n",
      "Epoch 42 / 50, Loss: 31.0345, Validation Loss: 38.0919, Accuracy: 61.6465\n",
      "Epoch 43 / 50, Loss: 34.4828, Validation Loss: 38.2185, Accuracy: 61.5639\n",
      "Epoch 44 / 50, Loss: 30.4426, Validation Loss: 38.2817, Accuracy: 61.5019\n",
      "Epoch 45 / 50, Loss: 55.1724, Validation Loss: 37.8072, Accuracy: 62.0287\n",
      "Epoch 46 / 50, Loss: 51.7241, Validation Loss: 45.8376, Accuracy: 54.0750\n",
      "Epoch 47 / 50, Loss: 44.8277, Validation Loss: 45.6215, Accuracy: 54.2609\n",
      "Epoch 48 / 50, Loss: 45.8563, Validation Loss: 42.7806, Accuracy: 57.0292\n",
      "Epoch 49 / 50, Loss: 27.5862, Validation Loss: 38.3293, Accuracy: 61.5019\n",
      "Epoch 50 / 50, Loss: 44.8276, Validation Loss: 39.1236, Accuracy: 60.7375\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "model = Classifier(input_dim=23).to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=2e-5)\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs.detach_()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            prediction = (outputs > 0.5).float()\n",
    "            correct += (prediction == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "    val_loss /= len(test_loader)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1} / {num_epochs}, Loss: {loss.item():.4f}, Validation Loss: {val_loss:.4f}, Accuracy: {float(correct) / float(total) * 100:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

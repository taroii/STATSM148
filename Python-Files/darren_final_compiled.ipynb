{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Compiled Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f2977b2f30>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(2024)\n",
    "np.random.seed(2024)\n",
    "torch.manual_seed(2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaler(col):\n",
    "    return (col - np.min(col)) / (np.max(col) - np.min(col))\n",
    "\n",
    "\n",
    "def vec_to_list(event_list):\n",
    "    event_list = event_list.replace('[', '').replace(']', '').split()\n",
    "    event_list = [int(float(x)) for x in event_list]\n",
    "    return event_list\n",
    "\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, embedding_dim, n_events=5):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(n_events, embedding_dim, bias=False)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "def preprocessing_steps(data):\n",
    "    df = data.copy()\n",
    "    df = df.drop(columns=['first_20_events', 'time_since_last_event', 'Unnamed: 1', 'downpayment_cleared', 'first_purchase',\n",
    "                          'max_milestone', 'downpayment_received', 'account_activitation', 'customer_id'])\n",
    "    \n",
    "    df = df.dropna(axis=0)\n",
    "    df_0, df_1 = df[df.order_ships == 0], df[df.order_ships == 1]\n",
    "    df_0 = df_0.sample(n=len(df_1), random_state=2024)\n",
    "    # df_1 = df_1.sample(n=(len(df_0)), replace=True)\n",
    "    df_balanced = pd.concat([df_0, df_1], axis=0).reset_index(drop=True)\n",
    "\n",
    "    # shuffle\n",
    "    df_balanced = df_balanced.sample(frac=1)\n",
    "\n",
    "    df_X = df_balanced.drop(columns='order_ships')\n",
    "    target = df_balanced.order_ships\n",
    "    ori_df = df.drop(columns='order_ships')\n",
    "    ori_target = df.order_ships\n",
    "\n",
    "    boolean_col = ['discover', 'one_more_journey', 'approved_credit', 'has_prospecting', 'has_pre_application']\n",
    "\n",
    "    for col in boolean_col:\n",
    "        df_X[col] = [1 if val == True else 0 for val in df_X[col]]\n",
    "        ori_df[col] = [1 if val == True else 0 for val in ori_df[col]]\n",
    "\n",
    "\n",
    "    return ori_df, ori_target, df_X, target\n",
    "\n",
    "\n",
    "def preprocessing_steps_embedding(data):\n",
    "    df = data.copy()\n",
    "    df = df.drop(columns=['Unnamed: 1', 'downpayment_cleared', 'first_purchase',\n",
    "                          'max_milestone', 'downpayment_received', 'account_activitation', 'customer_id'])\n",
    "    df = df.astype({col: 'float' for col in df.columns[:-2]})\n",
    "    \n",
    "    df = df.dropna(axis=0)\n",
    "\n",
    "\n",
    "    events = df['first_20_events'].apply(vec_to_list)\n",
    "    events = torch.tensor(events.to_list()).float()\n",
    "    events_emb = Embedding(5, 20)\n",
    "    events = events_emb(events)\n",
    "    event_pd = pd.DataFrame(events.detach().numpy(), columns=[f'event_id_{i}' for i in range(5)])\n",
    "\n",
    "    time = df['time_since_last_event'].apply(vec_to_list)\n",
    "    time = torch.tensor(time.to_list()).float()\n",
    "    time_emb = Embedding(5, 20)\n",
    "    time = time_emb(time)\n",
    "    time_pd = pd.DataFrame(time.detach().numpy(), columns=[f'time_{i}' for i in range(5)])\n",
    "    df = pd.concat([df, event_pd, time_pd], axis=1)\n",
    "    df = df.drop(columns=['first_20_events', 'time_since_last_event'])\n",
    "\n",
    "\n",
    "    df_0, df_1 = df[df.order_ships == 0], df[df.order_ships == 1]\n",
    "    df_0 = df_0.sample(n=len(df_1), random_state=2024)\n",
    "    # df_1 = df_1.sample(n=(len(df_0)), replace=True)\n",
    "    df_balanced = pd.concat([df_0, df_1], axis=0).reset_index(drop=True)\n",
    "\n",
    "    # shuffle\n",
    "    df_balanced = df_balanced.sample(frac=1)\n",
    "\n",
    "    df_X = df_balanced.drop(columns='order_ships')\n",
    "    target = df_balanced.order_ships\n",
    "    ori_df = df.drop(columns='order_ships')\n",
    "    ori_target = df.order_ships\n",
    "\n",
    "    boolean_col = ['discover', 'one_more_journey', 'approved_credit', 'has_prospecting', 'has_pre_application']\n",
    "\n",
    "    for col in boolean_col:\n",
    "        df_X[col] = [1 if val == True else 0 for val in df_X[col]]\n",
    "        ori_df[col] = [1 if val == True else 0 for val in ori_df[col]]\n",
    "\n",
    "\n",
    "    return ori_df, ori_target, df_X, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dfx = pd.read_csv('data_with_embeddings.csv')\n",
    "new_dfx = new_dfx.dropna(axis=0)\n",
    "num_cols = ['num_journeys', 'max_journey', 'number_accounts', 'average_length_seq', \n",
    "            'time_in_discover', 'time_in_apply', 'time_max', 'time_mean', 'time_std', \n",
    "            'total_time_spent', 'event_embd_0', 'event_embd_1', 'event_embd_2', \n",
    "            'event_embd_3','event_embd_4', 'time_embd_0', 'time_embd_1', 'time_embd_2',\n",
    "            'time_embd_3', 'time_embd_4']\n",
    "\n",
    "categorical_cols = ['most_repeated_event', 'initial_device']\n",
    "boolean_cols = ['discover', 'one_more_journey', 'approved_credit', 'has_prospecting', 'has_pre_application']\n",
    "target = 'order_ships'\n",
    "X_train, X_test, y_train, y_test = train_test_split(new_dfx.drop(columns='order_ships'), \n",
    "                                                    new_dfx.order_ships, \n",
    "                                                    test_size=0.2,\n",
    "                                                    stratify=new_dfx.order_ships,\n",
    "                                                    random_state=2024)\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "\n",
    "numerical = X_train.loc[:,num_cols]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(numerical)\n",
    "numerical_tran = scaler.transform(numerical)\n",
    "numerical_test = scaler.transform(X_test[num_cols])\n",
    "numerical_tr = pd.DataFrame(numerical_tran, columns=num_cols)\n",
    "numerical_ts = pd.DataFrame(numerical_test, columns=num_cols)\n",
    "\n",
    "X_train = pd.concat([numerical_tr, X_train[categorical_cols], X_train[boolean_cols]] , axis=1)\n",
    "X_test = pd.concat([numerical_ts, X_test[categorical_cols], X_test[boolean_cols]] , axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = new_dfx['order_ships'].reset_index(drop=True)\n",
    "X = new_dfx.drop(columns=['order_ships']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val(clf, X, y):\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=2024)\n",
    "\n",
    "    f1_scores = []\n",
    "    accuracy_scores = []\n",
    "\n",
    "    for train_index, test_index in cv.split(X, y):\n",
    "\n",
    "        X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        X_train = X_train.reset_index(drop=True)\n",
    "        X_test = X_test.reset_index(drop=True)\n",
    "        y_train = y_train.reset_index(drop=True)\n",
    "        y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "        numerical = X_train.loc[:,num_cols]\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(numerical)\n",
    "\n",
    "        numerical_tran = scaler.transform(numerical)\n",
    "        numerical_test = scaler.transform(X_test[num_cols])\n",
    "        numerical_tr = pd.DataFrame(numerical_tran, columns=num_cols)\n",
    "        numerical_ts = pd.DataFrame(numerical_test, columns=num_cols)\n",
    "\n",
    "        X_train = pd.concat([numerical_tr, X_train[categorical_cols], X_train[boolean_cols]] , axis=1)\n",
    "        X_test = pd.concat([numerical_ts, X_test[categorical_cols], X_test[boolean_cols]] , axis=1)\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        f1_scores.append(f1_score(y_test, y_pred))\n",
    "        accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "    print('F1 Score:', np.mean(f1_scores))\n",
    "    print('F1 Score:', np.std(f1_scores))\n",
    "    print('Accuracy:', np.mean(accuracy_scores))\n",
    "    print('Accuracy:', np.std(accuracy_scores)) \n",
    "\n",
    "    return np.mean(f1_scores), np.std(f1_scores), np.mean(accuracy_scores), np.std(accuracy_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8317192046775196\n",
      "0.49906583828722123\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=5, random_state=2024)\n",
    "dt.fit(X_train, y_train)\n",
    "dt_prediction = dt.predict(X_test)\n",
    "print(accuracy_score(y_test, dt_prediction))\n",
    "print(f1_score(y_test, dt_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = AdaBoostClassifier(n_estimators=200, learning_rate=.1, random_state=2024)\n",
    "ada.fit(X_train, y_train)\n",
    "ada_prediction = ada.predict(X_test)\n",
    "print(accuracy_score(y_test, ada_prediction))\n",
    "print(f1_score(y_test, ada_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier(n_estimators=200, learning_rate=0.1, random_state=2024)\n",
    "gb.fit(X_train, y_train)\n",
    "gb_prediction = gb.predict(X_test)\n",
    "print(accuracy_score(y_test, gb_prediction))\n",
    "print(f1_score(y_test, gb_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.884383342792878\n",
      "0.6653919694072659\n"
     ]
    }
   ],
   "source": [
    "lgbm = LGBMClassifier(n_estimators=200,\n",
    "                      max_depth=10, \n",
    "                      learning_rate=0.1,\n",
    "                      objective='binary',\n",
    "                      verbose=-1,\n",
    "                      random_state=2024)\n",
    "\n",
    "lgbm.fit(X_train, y_train)\n",
    "lgbm_prediction = lgbm.predict(X_test)\n",
    "print(accuracy_score(y_test, lgbm_prediction))\n",
    "print(f1_score(y_test, lgbm_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8916296543212101\n",
      "0.6991621858196825\n"
     ]
    }
   ],
   "source": [
    "xgb_clf = XGBClassifier(n_estimators=200,\n",
    "                        max_depth=10,\n",
    "                        learning_rate=0.1,\n",
    "                        tree_method='hist',\n",
    "                        objective='binary:logistic')\n",
    "\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "xgb_pred = xgb_clf.predict(X_test)\n",
    "print(accuracy_score(y_test, xgb_pred))\n",
    "print(f1_score(y_test, xgb_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(xgb_clf.feature_names_in_, xgb_clf.feature_importances_)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Feature Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'verbosity': 0,\n",
    "        'objective': 'binary:logistic',\n",
    "        'random_state': 2024,\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 400),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 20),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0, 10.0),\n",
    "        'scale_pos_weight': trial.suggest_float('scale_pos_weight', 1.0, 10.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 2.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1.0, 5.0)\n",
    "    }\n",
    "\n",
    "    model = XGBClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    prediction = model.predict(X_test)\n",
    "    f1 = f1_score(y_test, prediction)\n",
    "\n",
    "    return f1\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 354,\n",
       " 'max_depth': 10,\n",
       " 'learning_rate': 0.09777273210428525,\n",
       " 'subsample': 0.7355818769405801,\n",
       " 'gamma': 3.513270943451698,\n",
       " 'scale_pos_weight': 1.7901629274544835,\n",
       " 'reg_alpha': 0.8063538782719757,\n",
       " 'reg_lambda': 3.8289679315225817}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8832812307618387\n",
      "0.7293478639323143\n"
     ]
    }
   ],
   "source": [
    "xgb_clf = XGBClassifier(**study.best_params,\n",
    "                        tree_method='hist',\n",
    "                        objective='binary:logistic')\n",
    "\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "xgb_pred = xgb_clf.predict(X_test)\n",
    "print(accuracy_score(y_test, xgb_pred))\n",
    "print(f1_score(y_test, xgb_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[241761,  27111],\n",
       "       [ 11756,  52369]], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, xgb_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.7294253743270663\n",
      "F1 Score: 0.0008007273983766514\n",
      "Accuracy: 0.8833260659403528\n",
      "Accuracy: 0.0004522341352494301\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Cross validation\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m f1_mean, f1_std, acc_mean, acc_std \u001b[38;5;241m=\u001b[39m cross_val(xgb_clf, X, y)\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "# Cross validation\n",
    "\n",
    "f1_mean, f1_std, acc_mean, acc_std = cross_val(xgb_clf, X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

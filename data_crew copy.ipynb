{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STATS M148 Final Project Code Notebook\n",
    "\n",
    "### Team Name : Data Crew\n",
    "\n",
    "**Contributions:**\n",
    "* Axel Malvaez \n",
    "* Darren Hsieh\n",
    "* Ryan Kawamura\n",
    "* Taro Iyadomi  \n",
    "* Dan Oâ€™Brien\n",
    "\n",
    "This is the compilation of all code necessary to obtain the results found in our final report. Necesary files include: \n",
    "- Original dataset\n",
    "- Event definitions dataset\n",
    "- Smaller sample dataset (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_correct_sequence(s):\n",
    "    \"\"\"Function that checks if the sequence is correct.\n",
    "\n",
    "    Args:\n",
    "        s (list of ints): This is the journey steps until end variable in the Fingerhut data.\n",
    "\n",
    "    Returns:\n",
    "        Bool : This is a boolean that is True if the sequence is correct and False if it is not.\n",
    "    \"\"\"\n",
    "    \n",
    "    s = list(s)\n",
    "    temp = s[0]\n",
    "    for i in range(1, len(s)):\n",
    "        if s[i] == temp+1:\n",
    "            temp = s[i]\n",
    "        elif s[i] == 1:\n",
    "            temp = 1\n",
    "        else :\n",
    "            print('error because temp is ', temp, ' and x[i] is ', s[i])\n",
    "            print('i is ', i)\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def correct_sequences(s):\n",
    "    \"\"\"Function that corrects the sequences (journey steps until end) in the Fingerhut data.\n",
    "\n",
    "    Args:\n",
    "        s (list of ints): This is the journey steps until end variable in the Fingerhut data.\n",
    "\n",
    "    Returns:\n",
    "        seq : This is the corrected journey steps until end variable.\n",
    "    \"\"\"\n",
    "    seq = list(s)\n",
    "    temp = s[0]\n",
    "    for i in range(1, len(seq)):\n",
    "        # if 1 then start again\n",
    "        if seq[i] == 1:\n",
    "            temp = 1\n",
    "        elif seq[i] == temp+1:\n",
    "            temp = seq[i]\n",
    "        else :\n",
    "            seq[i] = temp+1\n",
    "            temp = seq[i]\n",
    "    return seq\n",
    "\n",
    "def fingerhut_data_cleaner(og_df, defs):\n",
    "    \"\"\"\n",
    "    Function to drop duplicates, reindex journey steps, convert timestamps, and merge event definitions.\n",
    "\n",
    "    args:\n",
    "     - og_df: This is the original Fingerhut data\n",
    "     - defs: This is the Event Definitions data frame (also provided by fingerhut)\n",
    "    \n",
    "    output:\n",
    "     - df: This is the cleaned Fingerhut data\n",
    "    \"\"\"\n",
    "    # Dropping duplicate (ignoring journey steps variable)\n",
    "    df = og_df[['customer_id',\n",
    "             'account_id',\n",
    "             'ed_id',\n",
    "             'event_name',\n",
    "             'event_timestamp',\n",
    "             'journey_steps_until_end',\n",
    "             'journey_id',\n",
    "             'milestone_number',]]\n",
    "    \n",
    "    # Filling in missing milestone numbers with 0\n",
    "    df.loc[:,['milestone_number']] = df['milestone_number'].copy().fillna(0)\n",
    "\n",
    "    df = df.drop_duplicates(subset=['customer_id', 'account_id', 'ed_id', 'event_name', 'event_timestamp'])\n",
    "    df = df.reset_index(drop=True) # re-indexing\n",
    "\n",
    "    # Re-adding journey_steps_until_end (Axel's way)\n",
    "    j_steps = df['journey_steps_until_end']\n",
    "    s_corrected = correct_sequences(j_steps)\n",
    "    df['journey_steps_until_end'] = s_corrected\n",
    "\n",
    "    # Convert event_timestamps to datetime objects\n",
    "    df['event_timestamp'] = pd.to_datetime(df['event_timestamp'], format='mixed')\n",
    "    \n",
    "    # Adding a `stage` variable based on the event definitions\n",
    "    df_stages = defs[['event_name', 'stage']]\n",
    "    \n",
    "    df = pd.merge(df, df_stages, on ='event_name', how = 'left')\n",
    "    \n",
    "    # Setting positive values for account_ids\n",
    "    df['account_id'] = remove_if(df, 'account_id')\n",
    "\n",
    "    # Setting positive values for customer_ids\n",
    "    df['customer_id'] = remove_if(df, 'customer_id')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_n_accounts(df):\n",
    "    \"\"\"\n",
    "    Adds a new column representing the number of accounts each customer has.    \n",
    "    \"\"\"\n",
    "    # Counting the unique number of account_ids for each customer_id\n",
    "    unique_account_counts = df.groupby('customer_id')['account_id'].nunique().reset_index(name='n_accounts')\n",
    "\n",
    "    # Merging the unique account counts back into the original dataframe\n",
    "    return pd.merge(df, unique_account_counts, on='customer_id')\n",
    "\n",
    "def add_has_discover(df):\n",
    "    \"\"\"\n",
    "    Adds a new column representing whether a customer has gone through the 'Discover' phase.\n",
    "    \"\"\"\n",
    "    discover_customers = df.groupby('customer_id')['stage'].apply(lambda x: 'Discover' in x.values).reset_index(name='has_discover')\n",
    "\n",
    "    return pd.merge(df, discover_customers, on='customer_id')\n",
    "\n",
    "def add_has_first_purchase(df):\n",
    "    \"\"\"\n",
    "    Adds a new column representing whether a customer has made their first purchase. \n",
    "    \n",
    "    WE ARE ADDING THE BOOLEAN VALUE WITHOUT TAKING CARE IF IT WAS A MILESTONE OR NOT I.E\n",
    "    WE ARE NOT TAKING INTO ACCOUNT THAT 'FIRST PURCHASE' COULD BE JUST BROWSING PRODUCTS AND NOT\n",
    "    ACTUALLY BUYING SOMETHING\n",
    "\n",
    "    \"\"\"\n",
    "    first_purchase_customers = df.groupby('customer_id')['stage'].apply(lambda x: 'First Purchase' in x.values).reset_index(name='has_first_purchase')\n",
    "\n",
    "    return pd.merge(df, first_purchase_customers, on='customer_id')\n",
    "\n",
    "def split_sequences(df):\n",
    "    \"\"\"Function that given the dataframe, returns a list of lists with the sequences of events\n",
    "\n",
    "    Args:\n",
    "        df (dataframe): The dataframe with the data\n",
    "\n",
    "    Returns:\n",
    "        result: A list of lists with the sequences of events\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    current_sequence = []\n",
    "    \n",
    "    for idx, step in enumerate(df['journey_steps_until_end']):\n",
    "        if step == 1:\n",
    "            # If the list is not empty, i.e. we have a new 1 in\n",
    "            # the journey we append the current sequence to the result\n",
    "            if current_sequence:\n",
    "                result.append(current_sequence)\n",
    "            current_sequence = [df['ed_id'].iloc[idx]]\n",
    "        else:\n",
    "            current_sequence.append(df['ed_id'].iloc[idx])\n",
    "    \n",
    "    # In case the last sequence is not empty we append the remaining sequence\n",
    "    if current_sequence:\n",
    "        result.append(current_sequence)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def remove_if(df, col_name):\n",
    "    \"\"\"Function that removes the negative number of a customer_id\n",
    "\n",
    "    Args:\n",
    "        a (str): number of a customer_id\n",
    "\n",
    "    Returns:\n",
    "        str : number of a customer_id without the negative sign\n",
    "    \"\"\"\n",
    "    values = df[col_name].apply(lambda x : (-1)*x if x < 0 else x).astype('int64')\n",
    "    return values\n",
    "\n",
    "def number_journeys_and_max(cus_df):\n",
    "    \"\"\"Function to check the number of journeys in a sequence\n",
    "\n",
    "    Args:\n",
    "        seq (list): List of values\n",
    "\n",
    "    Returns:\n",
    "        int: Number of journeys in the sequence\n",
    "    \"\"\"\n",
    "    j_steps = cus_df['journey_steps_until_end']\n",
    "    ones = [i for i, x in enumerate(j_steps) if x == 1]\n",
    "    return len(ones), max(j_steps)\n",
    "\n",
    "def has_discover(cust_df):\n",
    "    \"\"\"Function to check if a sequence has the discovery event\n",
    "\n",
    "    Args:\n",
    "        cust_df (pd.DataFrame): Dataset of a certain customer (not all the dataset, just one customer)\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the sequence has the discovery event, False otherwise\n",
    "    \"\"\"\n",
    "    return 'Discover' in list(cust_df['stage'])\n",
    "\n",
    "def number_accounts(cust_df):\n",
    "    \"\"\"Function to add the number of accounts to the dataset\n",
    "\n",
    "    Args:\n",
    "        cust_df (pd.DataFrame): Dataset of a certain customer (not all the dataset, just one customer)\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataset with the number of accounts in a new column\n",
    "    \"\"\"\n",
    "    return cust_df['account_id'].nunique()\n",
    "\n",
    "def has_more_one_journey(j_steps):\n",
    "    \"\"\"Function to check if a sequence has repeated values\n",
    "\n",
    "    Args:\n",
    "        seq (list): List of values\n",
    "\n",
    "    Returns:\n",
    "        bool: True if there are repeated values, False otherwise\n",
    "    \"\"\"\n",
    "    return len(j_steps) != len(set(j_steps))\n",
    "\n",
    "def most_repeated_event(cust_df):\n",
    "    \"\"\"Function that returns the most repeated event in a sequence\n",
    "\n",
    "    Args:\n",
    "        cust_df (pd.DataFrame): Dataset of a certain customer (not all the dataset, just one customer)\n",
    "\n",
    "    Returns:\n",
    "        str: The most repeated event in the sequence\n",
    "    \"\"\"\n",
    "    return cust_df['ed_id'].mode()[0]\n",
    "\n",
    "def average_length_seq(cust_df):\n",
    "    \"\"\"Function to add the average length of the sequences to the dataset\n",
    "\n",
    "    Args:\n",
    "        cust_df (pd.DataFrame): Dataset of a certain customer (not all the dataset, just one customer)\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataset with the average length of the sequences in a new column\n",
    "    \"\"\"\n",
    "    new_df = cust_df.copy()\n",
    "    # Split the sequences\n",
    "    sequences = split_sequences(new_df)\n",
    "    return np.mean([len(seq) for seq in sequences])\n",
    "\n",
    "def has_prospecting(cust_df):\n",
    "    \"\"\"Function to check if a sequence has the prospecting event\n",
    "\n",
    "    Args:\n",
    "        cust_df (pd.DataFrame): Dataset of a certain customer (not all the dataset, just one customer)\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the sequence has the prospecting event, False otherwise\n",
    "    \"\"\"\n",
    "    evnts = list(cust_df['ed_id'])\n",
    "    return 20 in evnts or 21 in evnts or 24 in evnts\n",
    "\n",
    "def has_pre_application(cust_df):\n",
    "    \"\"\"Function to check if a sequence has the pre-application event\n",
    "\n",
    "    Args:\n",
    "        cust_df (pd.DataFrame): Dataset of a certain customer (not all the dataset, just one customer)\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the sequence has the pre-application event, False otherwise\n",
    "    \"\"\"\n",
    "    return 22 in list(cust_df['ed_id'])\n",
    "\n",
    "def initial_device(cust_df):\n",
    "    \"\"\"Function to get the initial device of a customer\n",
    "    \"\"\"\n",
    "    events = set(cust_df['event_name'])\n",
    "    phone = ['phone' in event for event in events]\n",
    "    web = ['web' in event for event in events]\n",
    "    \n",
    "    if np.array(phone).any() and np.array(web).any():\n",
    "        return 3\n",
    "    elif np.array(phone).any():\n",
    "        return 1\n",
    "    elif np.array(web).any():\n",
    "        return 2\n",
    "    \n",
    "def has_approved(cust_df):\n",
    "    \"\"\"Function to check if a sequence has the approved event\n",
    "\n",
    "    Args:\n",
    "        cust_df (pd.DataFrame): Dataset of a certain customer (not all the dataset, just one customer)\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the sequence has the approved event, False otherwise\n",
    "    \"\"\"\n",
    "    x = set(cust_df['ed_id'])\n",
    "    return 15 in x or 12 in x\n",
    "\n",
    "def get_first_n_events(cust_df, n = 10):\n",
    "    \"\"\"Function that returns the first 10 events of a sequence\n",
    "\n",
    "    Args:\n",
    "        cust_df (pd.DataFrame): Dataset of a certain customer (not all the dataset, just one customer)\n",
    "\n",
    "    Returns:\n",
    "        list: The first 10 events of the sequence, padded with np.nan if necessary\n",
    "    \"\"\"\n",
    "    events = cust_df['ed_id'].head(n).tolist()\n",
    "    # Pad with np.nan if the sequence has fewer than 10 events\n",
    "    events += [0] * (n - len(events))\n",
    "    return np.array(events)\n",
    "\n",
    "def get_time_since_last_event(cust_df, n=10):\n",
    "    cust_df = cust_df.head(n)\n",
    "    x = cust_df.groupby(['customer_id', 'journey_id'])['event_timestamp'].diff()\n",
    "    x = x.fillna(pd.Timedelta(seconds=0))\n",
    "    x = x.dt.total_seconds()\n",
    "    x = x.tolist() + [0] * (n - len(x))\n",
    "    return np.array(x)\n",
    "\n",
    "def which_milestones(cust_df):\n",
    "    \"\"\"Function that returns in a tuple in the following sequence the next statemens:\n",
    "    - If the customer has applied for credit and it has been approved (milestone 1)\n",
    "    - If the customer has first purchase (milestone 2)\n",
    "    - If the customer has account activitation (milestone 3)\n",
    "    - If the customer has downpayment received (milestone 4)\n",
    "    - If the customer has downpayment cleared (milestone 5)\n",
    "    - If the customer has order shipped (milestone 6)\n",
    "\n",
    "    Args:\n",
    "        cust_df (_type_): _description_\n",
    "    \"\"\"\n",
    "    milestones = set(cust_df['milestone_number'].unique())\n",
    "    max_milestone = max(milestones)\n",
    "    return (1 in milestones, 2 in milestones, 3 in milestones, 4 in milestones, 5 in milestones, 6 in milestones), max_milestone\n",
    "\n",
    "# Functions for time\n",
    "def get_idxs(cust_df, stage, milestone = -1):\n",
    "    \"\"\"Function to get the indexes of a certain stage\n",
    "\n",
    "    Args:\n",
    "        cust_df (pd.DataFrame): Dataset of a certain customer (not all the dataset, just one customer)\n",
    "\n",
    "    Returns:\n",
    "        list: List with the indexes of a certain stage\n",
    "    \"\"\"\n",
    "    if milestone != -1:\n",
    "        return list(cust_df[cust_df['milestone_number'] == milestone].index)\n",
    "    \n",
    "    return list(cust_df[cust_df['stage'] == stage].index)\n",
    "\n",
    "def time_in_discover(cust_df, seconds_differences):\n",
    "    \"\"\"Function to calculate the time between events\n",
    "\n",
    "    Args:\n",
    "        cust_df (pd.DataFrame): Dataset of a certain customer (not all the dataset, just one customer)\n",
    "\n",
    "    Returns:\n",
    "        list: List with the time between events\n",
    "    \"\"\"\n",
    "    idxs = get_idxs(cust_df, 'Discover')\n",
    "    \n",
    "    time_in = []\n",
    "    for idx in idxs:\n",
    "        if idx + 1 < len(seconds_differences):\n",
    "            time_in.append(seconds_differences[idx + 1])\n",
    "        else:\n",
    "            time_in.append(0)\n",
    "    return sum(time_in)\n",
    "\n",
    "def time_in_apply(cust_df, seconds_differences):\n",
    "    \"\"\"Function to calculate the time between events\n",
    "\n",
    "    Args:\n",
    "        cust_df (pd.DataFrame): Dataset of a certain customer (not all the dataset, just one customer)\n",
    "\n",
    "    Returns:\n",
    "        list: List with the time between events\n",
    "    \"\"\"\n",
    "    idxs = get_idxs(cust_df, 'Apply for Credit')\n",
    "    \n",
    "    time_in = []\n",
    "    for idx in idxs:\n",
    "        if idx + 1 < len(seconds_differences):\n",
    "            time_in.append(seconds_differences[idx + 1])\n",
    "        else:\n",
    "            time_in.append(0)\n",
    "    return sum(time_in)\n",
    "\n",
    "def time_reach_milestone1(cust_df, seconds_differences):\n",
    "    \"\"\"Function to calculate the time between events\n",
    "\n",
    "    Args:\n",
    "        cust_df (pd.DataFrame): Dataset of a certain customer (not all the dataset, just one customer)\n",
    "\n",
    "    Returns:\n",
    "        list: List with the time between events\n",
    "    \"\"\"\n",
    "    idxs = get_idxs(cust_df, 'Apply for Credit', 1)\n",
    "    \n",
    "    # sum all the times before the milestone\n",
    "    return sum(seconds_differences[1:idxs[0]+1])\n",
    "\n",
    "def group_by_approach(cust_df):\n",
    "    cust_df = cust_df.reset_index(drop=True)\n",
    "    # applying all the functions to get the data\n",
    "    num_journeys, max_journey = number_journeys_and_max(cust_df)\n",
    "    discover = has_discover(cust_df)\n",
    "    numb_accs = number_accounts(cust_df)\n",
    "    more_one_journey = has_more_one_journey(cust_df['journey_steps_until_end'])\n",
    "    repeated_event = most_repeated_event(cust_df)\n",
    "    avg_length_journey = average_length_seq(cust_df)\n",
    "    has_pros = has_prospecting(cust_df)\n",
    "    pre_applic = has_pre_application(cust_df)\n",
    "    device = initial_device(cust_df)\n",
    "    x = cust_df['event_timestamp'].diff().dt.total_seconds().tolist()\n",
    "    time_disc = time_in_discover(cust_df, x)\n",
    "    time_apply = time_in_apply(cust_df, x)\n",
    "    # time_milestone1 = time_reach_milestone1(cust_df, x)\n",
    "    \n",
    "    milestones, max_milestone = which_milestones(cust_df)\n",
    "    \n",
    "    # Creating the new data frame\n",
    "    new_df = pd.DataFrame({'num_journeys': num_journeys,\n",
    "                           'max_journey': max_journey,\n",
    "                           'discover': discover, \n",
    "                           'number_accounts': numb_accs,\n",
    "                           'one_more_journey': more_one_journey,\n",
    "                           'most_repeated_event': repeated_event,\n",
    "                           'average_length_seq': avg_length_journey,\n",
    "                           'approved_credit': milestones[0],\n",
    "                           'first_purchase': milestones[1],\n",
    "                           'account_activitation': milestones[2],\n",
    "                           'downpayment_received': milestones[3],\n",
    "                           'downpayment_cleared': milestones[4],\n",
    "                           'order_ships': milestones[5],\n",
    "                           'max_milestone': max_milestone,\n",
    "                            'has_prospecting': has_pros,\n",
    "                            'has_pre_application': pre_applic,\n",
    "                            'initial_device': device,\n",
    "                            'time_in_discover': time_disc,\n",
    "                            'time_in_apply': time_apply,\n",
    "                            #'time_reach_milestone_1': time_milestone1,\n",
    "                           'index':[0]})\n",
    "    return new_df    \n",
    "\n",
    "def get_classification_dataset(data, event_defs, n_events = 10):\n",
    "    \"\"\"This function is the one that gives you the final dataset with the features and the target variable\n",
    "\n",
    "    Args:\n",
    "        data (_type_): the original dataset (without any cleaning or anything like that)\n",
    "        event_defs (_type_): the event definitions dataset\n",
    "        n_events (int, optional): Number of events in the last column. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "        df : The final dataset with the features and the target variable\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    df = fingerhut_data_cleaner(data, event_defs)\n",
    "    # drop the promotion_created event\n",
    "    idxs = list(df[df['event_name'] == 'promotion_created'].index)\n",
    "    df.drop(idxs, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Grouping by the customer id and gathering the data\n",
    "    new_df = df.groupby('customer_id').apply(group_by_approach)\n",
    "    new_df.drop(columns=['index'], inplace=True)\n",
    "    \n",
    "    # Adding the first n events\n",
    "    x = list(df.groupby('customer_id').apply(get_first_n_events, n = n_events))\n",
    "    new_df['first_' + str(n_events) + '_events'] = x\n",
    "    \n",
    "    # Adding the time of this first n events\n",
    "    x = list(df.groupby('customer_id').apply(get_time_since_last_event, n = n_events))\n",
    "    new_df['time_since_last_event'] = x\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "def scaler(col):\n",
    "    \"\"\"Function to scale a column\"\"\"\n",
    "    return (col - np.min(col)) / (np.max(col) - np.min(col))\n",
    "\n",
    "# Generate Embeddings Function\n",
    "def embeddings(df, col_name = 'first_20_events'):\n",
    "    sequences_evs = df[col_name].apply(lambda x: np.array(x)).to_numpy()\n",
    "    sequences_times = df['time_since_last_event'].apply(lambda x: np.array(x)).to_numpy()\n",
    "\n",
    "    padded_time_events = np.vstack(sequences_evs)\n",
    "    padded_time_waits = np.vstack(sequences_times)\n",
    "    \n",
    "    max_seq_length = 20\n",
    "    embedding_dim = 5\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=128, input_shape=(max_seq_length, 1), return_sequences=False))  \n",
    "    model.add(Dense(units=64, activation='relu'))  \n",
    "    model.add(Dense(units=32, activation='relu'))  \n",
    "    model.add(Dense(units=embedding_dim, activation='relu'))  \n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "    print('Predicting embeddings for time...')\n",
    "    time_embeddings = model.predict(padded_time_waits)\n",
    "\n",
    "    print('Predicting embeddings for events...')\n",
    "    event_embeddings = model.predict(padded_time_events)\n",
    "\n",
    "    event_embd = pd.DataFrame(event_embeddings, columns=[f'event_embd_{i}' for i in range(5)])\n",
    "    time_embd = pd.DataFrame(time_embeddings, columns=[f'time_embd_{i}' for i in range(5)])\n",
    "\n",
    "    df.drop(columns=[col_name, 'time_since_last_event'], inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    new_dfx = pd.concat([df, time_embd, event_embd], axis=1)\n",
    "    return new_dfx\n",
    "\n",
    "def vec_to_list(event_list):\n",
    "    event_list = event_list.replace('[', '').replace(']', '').split()\n",
    "    event_list = [int(float(x)) for x in event_list]\n",
    "    return event_list\n",
    "\n",
    "def preprocessing_steps_embedding(data):\n",
    "    df = data.copy()\n",
    "    \n",
    "    # Dropping columns that introduce bias to the model\n",
    "    df = df.drop(columns=['Unnamed: 1', 'downpayment_cleared', 'first_purchase',\n",
    "                          'max_milestone', 'downpayment_received', 'account_activitation', 'customer_id'])\n",
    "    \n",
    "    # We set this parameters for future interactions with these features\n",
    "    number_events_fixed = 20\n",
    "    col_name = 'first_' + str(number_events_fixed) +'_events'\n",
    "    \n",
    "    # As we are reading the data from a csv, the list of events is read as a string\n",
    "    # and therefore we need to transform this type of data\n",
    "    result = []\n",
    "    for item in list(df[col_name]):\n",
    "        numbers = [int(num) for num in item.replace('[', '').replace(']', '').split()]\n",
    "        #numbers += [0] * (number_events_fixed - len(numbers))    \n",
    "        result.append(numbers)\n",
    "    result2 = []\n",
    "    for item in list(df['time_since_last_event']):\n",
    "        numbers = [float(num) for num in item.replace('[', '').replace(']', '').split()]\n",
    "        #numbers += [0] * (number_events_fixed - len(numbers))    \n",
    "        result2.append(numbers)\n",
    "    \n",
    "    # We have the columns again in a list type\n",
    "    df[col_name] = result\n",
    "    df['time_since_last_event'] = result2\n",
    "    \n",
    "    # Here we set all the float columns to numbers 0 or 1\n",
    "    df = df.astype({col: 'float' for col in df.columns[:-2]})\n",
    "    \n",
    "    # We realized the dataset in the initial_devices had nan values\n",
    "    df = df.dropna(axis=0)\n",
    "    \n",
    "    # Adding more features\n",
    "    df['total_time_spent'] = df['time_since_last_event'].apply(lambda x: np.sum(x))\n",
    "    df['time_mean'] = df['time_since_last_event'].apply(lambda x: np.mean(x))\n",
    "    print('mean added')\n",
    "    df['time_std'] = df['time_since_last_event'].apply(lambda x: np.std(x))\n",
    "    print('std added')\n",
    "    df['time_max'] = df['time_since_last_event'].apply(lambda x: np.max(x))\n",
    "    print('max added')\n",
    "    \n",
    "    # We create and generate the embeddings\n",
    "    # we drop the first_20_events and the time_since_last_event column\n",
    "    # but we kept the embeddings\n",
    "    df = embeddings(df)\n",
    "\n",
    "    # Getting the dataset balanced\n",
    "    df_0, df_1 = df[df.order_ships == 0], df[df.order_ships == 1]\n",
    "    df_0 = df_0.sample(n=len(df_1), random_state=2024)\n",
    "    # df_1 = df_1.sample(n=(len(df_0)), replace=True)\n",
    "    df_balanced = pd.concat([df_0, df_1], axis=0).reset_index(drop=True)\n",
    "\n",
    "    # shuffle\n",
    "    df_balanced = df_balanced.sample(frac=1)\n",
    "\n",
    "    df_X = df_balanced.drop(columns='order_ships')\n",
    "    target = df_balanced.order_ships\n",
    "    ori_df = df.drop(columns='order_ships')\n",
    "    ori_target = df.order_ships\n",
    "\n",
    "    boolean_col = ['discover', 'one_more_journey', 'approved_credit', 'has_prospecting', 'has_pre_application']\n",
    "\n",
    "    for col in boolean_col:\n",
    "        df_X[col] = [1 if val == True else 0 for val in df_X[col]]\n",
    "        ori_df[col] = [1 if val == True else 0 for val in ori_df[col]]\n",
    "\n",
    "    return ori_df, ori_target, df_X, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a) Small Data Modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "# Change filepaths to appropriate file locations\n",
    "data_filepath = './export.csv' # smalle_sample.csv for smaller version\n",
    "event_defs_filepath = './Event+Definitions.csv'\n",
    "\n",
    "data = pd.read_csv(data_filepath)   # This is the original data\n",
    "event_defs = pd.read_csv(event_defs_filepath)  # This is the event dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slight Feature Engineering\n",
    "df = fingerhut_data_cleaner(data, event_defs)\n",
    "idxs = list(df[df['event_name'] == 'promotion_created'].index)\n",
    "df.drop(idxs, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df = df.groupby('customer_id').apply(group_by_approach)\n",
    "df.drop(columns=['index'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b) Order-Shipped Pie Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate proportions\n",
    "proportions = df['order_ships'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Plotting using a pie chart\n",
    "plt.figure(figsize=(8, 8))  \n",
    "patches, _, _ = plt.pie(proportions, labels=None, autopct='%1.1f%%', startangle=140)\n",
    "\n",
    "# Adding a legend\n",
    "plt.legend(patches, labels=proportions.index, loc=\"best\", fontsize = 14)\n",
    "\n",
    "# Adding a title\n",
    "plt.title('Proportion of Order Ships', fontsize = 18)\n",
    "\n",
    "# Save the plot as an image file\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.savefig('order_ships_pie_chart.png')  # Save as PNG file\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c) Max Journey Boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting using Seaborn with log scale\n",
    "plt.figure(figsize=(12, 9))  # Adjust the width and height as needed\n",
    "sns.histplot(df['max_journey'], bins=20, log_scale=True)  # Adjust bins as needed\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Max Journey Steps (Log Scale)', fontsize = 14)\n",
    "plt.ylabel('Frequency', fontsize = 14)\n",
    "plt.title('Histogram of Max Journey Steps (Log Scale)', fontsize = 18)\n",
    "\n",
    "# Show the plot\n",
    "plt.savefig('max_journey_histogram.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### d) Discover vs. Order Shipped Barplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = df.copy()\n",
    "plot_df['discover'] = plot_df['discover'].astype(int)\n",
    "plot_df['order_ships'] = plot_df['order_ships'].astype(int)\n",
    "\n",
    "# Calculate proportions manually\n",
    "proportions = plot_df.groupby('discover')['order_ships'].value_counts(normalize=True).unstack().fillna(0)\n",
    "\n",
    "# Plotting using Seaborn\n",
    "plt.figure(figsize=(8, 6))  # Adjust the width and height as needed\n",
    "bottom_bar = sns.barplot(data=proportions, x=proportions.index, y=proportions[1], color='blue', label='Order Ships')\n",
    "sns.barplot(data=proportions, x=proportions.index, y=proportions[0], color='orange', bottom=proportions[1], label='No Order Ships')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Discover', fontsize = 14)\n",
    "plt.ylabel('Proportion', fontsize = 14)\n",
    "plt.title('Relationship between Discover and Order Ships', fontsize = 18)\n",
    "\n",
    "# Show the legend with custom labels\n",
    "plt.legend(title='Order Status')\n",
    "\n",
    "# Set y-axis tick labels\n",
    "plt.xticks(ticks=[0, 1], labels=['False', 'True'])\n",
    "\n",
    "# Show the plot\n",
    "\n",
    "plt.savefig('relationship_discover_order_ships.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### e) YOY Analysis Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_shipped_df = df[df[\"event_name\"] == \"order_shipped\"]\n",
    "yoy_df = order_shipped_df.groupby(['year', 'week']).size().reset_index(name = 'order_shipped')\n",
    "yoy_df = yoy_df[0:len(yoy_df)-1] \n",
    "\n",
    "plt.figure(figsize=(18, 8))\n",
    "\n",
    "years = yoy_df['year'].unique()\n",
    "years.sort()\n",
    "\n",
    "for year in years:\n",
    "    year_df = yoy_df[yoy_df['year'] == year]\n",
    "    plt.plot(year_df['week'], year_df['order_shipped'], marker='o', label=str(year))\n",
    "\n",
    "plt.title('YOY Analysis of Orders Shipped by Week')\n",
    "plt.xlabel('Week')\n",
    "plt.ylabel('Number of Orders Shipped')\n",
    "plt.legend(title='Year', loc='upper left')\n",
    "plt.grid()\n",
    "plt.xticks(np.arange(1, 54))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a) Set Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b) Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add feature engineered variables (Appendix Table 1)\n",
    "# Change number_events_fixed to adjust dataset\n",
    "number_events_fixed = 20\n",
    "col_name = 'first_' + str(number_events_fixed) +'_events'\n",
    "\n",
    "df = get_classification_dataset(data, event_defs, n_events=number_events_fixed)\n",
    "df.reindex(sorted(df.columns), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index by customer_id\n",
    "cust_ids = df.index\n",
    "cust_ids = [x[0] for x in cust_ids]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df['customer_id'] = cust_ids\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c) Generating Embeddings and Balance Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run embedding and balancing\n",
    "ori_data, ori_target, df, target = preprocessing_steps_embedding(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data (clear memory if memory issues occur)\n",
    "dataset_filepath = './final_dataset.csv' # Change to your liking\n",
    "pd.concat([ori_data, ori_target], axis=1).to_csv(dataset_filepath, index=False) # This data was made in order to train the models and test with the whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a) Set Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2024)\n",
    "np.random.seed(2024)\n",
    "torch.manual_seed(2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b) Read In Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = './final_dataset.csv' # From Data Preparation part\n",
    "new_dfx = pd.read_csv(filepath)\n",
    "new_dfx = new_dfx.dropna(axis=0)\n",
    "num_cols = ['num_journeys', 'max_journey', 'number_accounts', 'average_length_seq', \n",
    "            'time_in_discover', 'time_in_apply', 'time_max', 'time_mean', 'time_std', \n",
    "            'total_time_spent', 'event_embd_0', 'event_embd_1', 'event_embd_2', \n",
    "            'event_embd_3','event_embd_4', 'time_embd_0', 'time_embd_1', 'time_embd_2',\n",
    "            'time_embd_3', 'time_embd_4']\n",
    "\n",
    "categorical_cols = ['most_repeated_event', 'initial_device']\n",
    "boolean_cols = ['discover', 'one_more_journey', 'approved_credit', 'has_prospecting', 'has_pre_application']\n",
    "target = 'order_ships'\n",
    "X_train, X_test, y_train, y_test = train_test_split(new_dfx.drop(columns='order_ships'), \n",
    "                                                    new_dfx.order_ships, \n",
    "                                                    test_size=0.2,\n",
    "                                                    stratify=new_dfx.order_ships,\n",
    "                                                    random_state=2024)\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "numerical = X_train.loc[:,num_cols]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(numerical)\n",
    "numerical_tran = scaler.transform(numerical)\n",
    "numerical_test = scaler.transform(X_test[num_cols])\n",
    "numerical_tr = pd.DataFrame(numerical_tran, columns=num_cols)\n",
    "numerical_ts = pd.DataFrame(numerical_test, columns=num_cols)\n",
    "\n",
    "X_train = pd.concat([numerical_tr, X_train[categorical_cols], X_train[boolean_cols]] , axis=1)\n",
    "X_test = pd.concat([numerical_ts, X_test[categorical_cols], X_test[boolean_cols]] , axis=1)\n",
    "\n",
    "y = new_dfx['order_ships'].reset_index(drop=True)\n",
    "X = new_dfx.drop(columns=['order_ships']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c) Preliminary Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation Function\n",
    "def cross_val(clf, X, y):\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=2024)\n",
    "\n",
    "    f1_scores = []\n",
    "    accuracy_scores = []\n",
    "\n",
    "    for train_index, test_index in cv.split(X, y):\n",
    "\n",
    "        X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        X_train = X_train.reset_index(drop=True)\n",
    "        X_test = X_test.reset_index(drop=True)\n",
    "        y_train = y_train.reset_index(drop=True)\n",
    "        y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "        numerical = X_train.loc[:,num_cols]\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(numerical)\n",
    "\n",
    "        numerical_tran = scaler.transform(numerical)\n",
    "        numerical_test = scaler.transform(X_test[num_cols])\n",
    "        numerical_tr = pd.DataFrame(numerical_tran, columns=num_cols)\n",
    "        numerical_ts = pd.DataFrame(numerical_test, columns=num_cols)\n",
    "\n",
    "        X_train = pd.concat([numerical_tr, X_train[categorical_cols], X_train[boolean_cols]] , axis=1)\n",
    "        X_test = pd.concat([numerical_ts, X_test[categorical_cols], X_test[boolean_cols]] , axis=1)\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        f1_scores.append(f1_score(y_test, y_pred))\n",
    "        accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "    print('F1 Score:', np.mean(f1_scores))\n",
    "    print('F1 Score:', np.std(f1_scores))\n",
    "    print('Accuracy:', np.mean(accuracy_scores))\n",
    "    print('Accuracy:', np.std(accuracy_scores)) \n",
    "\n",
    "    return np.mean(f1_scores), np.std(f1_scores), np.mean(accuracy_scores), np.std(accuracy_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "log_clf = LogisticRegression(random_state=2024)\n",
    "log_clf.fit(X_train, y_train)\n",
    "prediction = log_clf.predict(X_test)\n",
    "accuracy_score(y_test, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Trees\n",
    "dt = DecisionTreeClassifier(max_depth=5, random_state=2024)\n",
    "dt.fit(X_train, y_train)\n",
    "dt_prediction = dt.predict(X_test)\n",
    "print(accuracy_score(y_test, dt_prediction))\n",
    "print(f1_score(y_test, dt_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ada Boost\n",
    "ada = AdaBoostClassifier(n_estimators=200, learning_rate=.1, random_state=2024)\n",
    "ada.fit(X_train, y_train)\n",
    "ada_prediction = ada.predict(X_test)\n",
    "print(accuracy_score(y_test, ada_prediction))\n",
    "print(f1_score(y_test, ada_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient boosting\n",
    "gb = GradientBoostingClassifier(n_estimators=200, learning_rate=0.1, random_state=2024)\n",
    "gb.fit(X_train, y_train)\n",
    "gb_prediction = gb.predict(X_test)\n",
    "print(accuracy_score(y_test, gb_prediction))\n",
    "print(f1_score(y_test, gb_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LGBM\n",
    "lgbm = LGBMClassifier(n_estimators=200,\n",
    "                      max_depth=10, \n",
    "                      learning_rate=0.1,\n",
    "                      objective='binary',\n",
    "                      verbose=-1,\n",
    "                      random_state=2024)\n",
    "\n",
    "lgbm.fit(X_train, y_train)\n",
    "lgbm_prediction = lgbm.predict(X_test)\n",
    "print(accuracy_score(y_test, lgbm_prediction))\n",
    "print(f1_score(y_test, lgbm_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "xgb_clf = XGBClassifier(n_estimators=200,\n",
    "                        max_depth=10,\n",
    "                        learning_rate=0.1,\n",
    "                        tree_method='hist',\n",
    "                        objective='binary:logistic',\n",
    "                        random_state=2024)\n",
    "\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "xgb_pred = xgb_clf.predict(X_test)\n",
    "print(accuracy_score(y_test, xgb_pred))\n",
    "print(f1_score(y_test, xgb_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Confusion matrix\n",
    "cm = confusion_matrix(y_test, xgb_pred)\n",
    "matrix = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=xgb_clf.classes_)\n",
    "matrix.plot()\n",
    "plt.title('Confusion Matrix of XGBoost Before Tuning')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### d) XGBoost Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'verbosity': 0,\n",
    "        'objective': 'binary:logistic',\n",
    "        'random_state': 2024,\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 400),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 20),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0, 10.0),\n",
    "        'scale_pos_weight': trial.suggest_float('scale_pos_weight', 1.0, 10.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 2.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1.0, 5.0)\n",
    "    }\n",
    "\n",
    "    model = XGBClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    prediction = model.predict(X_test)\n",
    "    f1 = f1_score(y_test, prediction)\n",
    "\n",
    "    return f1\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'n_estimators': 354,\n",
    "               'max_depth': 10,\n",
    "               'learning_rate': 0.09777273210428525,\n",
    "               'subsample': 0.7355818769405801,\n",
    "               'gamma': 3.513270943451698,\n",
    "               'scale_pos_weight': 1.7901629274544835,\n",
    "               'reg_alpha': 0.8063538782719757,\n",
    "               'reg_lambda': 3.8289679315225817}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train new XGBoost model\n",
    "xgb_clf = XGBClassifier(**best_params,\n",
    "                        tree_method='hist',\n",
    "                        objective='binary:logistic',\n",
    "                        random_state=2024)\n",
    "\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "xgb_pred = xgb_clf.predict(X_test)\n",
    "print(accuracy_score(y_test, xgb_pred))\n",
    "print(f1_score(y_test, xgb_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for tuned XGBoost model\n",
    "cm = confusion_matrix(y_test, xgb_pred)\n",
    "matrix = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=xgb_clf.classes_)\n",
    "matrix.plot()\n",
    "plt.title('Confusion Matrix of Tune XGBoost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(xgb_clf.feature_names_in_, xgb_clf.feature_importances_)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Tuned XGBoost Feature Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### e) Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_nn, X_val_nn, y_train_nn, y_val_nn = train_test_split(X_train, \n",
    "                                                              y_train, \n",
    "                                                              test_size=0.2,\n",
    "                                                              stratify=y_train,\n",
    "                                                              random_state=2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "print(\"Class weights:\", class_weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(27,)),\n",
    "    Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    Dropout(0.3),  # Dropout layer to prevent overfitting\n",
    "    Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    Dropout(0.5),  # Dropout layer to prevent overfitting\n",
    "    Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    Dropout(0.2),  # Dropout layer to prevent overfitting\n",
    "    Dense(31, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.1)),\n",
    "    #Dropout(0.),  # Dropout layer to prevent overfitting\n",
    "    Dense(1, activation='sigmoid')  # Output layer with sigmoid activation for binary classification\n",
    "])\n",
    "\n",
    "learning_rate = 0.001 \n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Compile the model_1\n",
    "model_1.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model_1 summary\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "history = model_1.fit(X_train_nn, y_train_nn, \n",
    "                    epochs=5, \n",
    "                    batch_size=128,\n",
    "                    validation_data=(X_val_nn, y_val_nn),\n",
    "                    class_weight=class_weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = model_1.predict(X_test)\n",
    "\n",
    "# Convert predictions to binary labels\n",
    "binary_predictions = (predictions > 0.5).astype(int)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, binary_predictions.reshape(-1))\n",
    "print('Accuracy:', accuracy)\n",
    "\n",
    "f1 = f1_score(y_test, binary_predictions.reshape(-1))\n",
    "print('F1 Score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 6))\n",
    "sns.heatmap(confusion_matrix(y_test, binary_predictions.reshape(-1)), annot=True, fmt='g', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Neural Network Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes the analysis and predictive modeling code used for our STATS M148 2024 project. Additional plots and tables were generated using the results above in external software like Tableau and LaTeX. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "truthsystems",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
